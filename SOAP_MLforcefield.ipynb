{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ase\n",
    "from ase.calculators.lj import LennardJones\n",
    "import matplotlib.pyplot as plt\n",
    "import dscribe\n",
    "import dscribe.descriptors\n",
    "from dscribe.descriptors import SOAP\n",
    "from ase.visualize import view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1)\n",
      "[  0   6  13  20  27  34  41  48  54  61  68  75  82  89  96 102 109 116\n",
      " 123 130 137 144 150 157 164 171 178 185 192 199]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# Load the dataset\n",
    "D_numpy = np.load(\"D.npy\")[:, 0, :]  # We only have one SOAP center\n",
    "n_samples, n_features = D_numpy.shape\n",
    "E_numpy = np.array([np.load(\"E.npy\")]).T\n",
    "print(E_numpy.shape)\n",
    "F_numpy = np.load(\"F.npy\")\n",
    "dD_dr_numpy = np.load(\"dD_dr.npy\")[:, 0, :, :, :]  # We only have one SOAP center\n",
    "r_numpy = np.load(\"r.npy\")\n",
    "\n",
    "\n",
    "# Select equally spaced points for training\n",
    "n_train = 30\n",
    "idx = np.linspace(0, len(r_numpy) - 1, n_train).astype(int)\n",
    "print(idx)\n",
    "D_train_full = D_numpy[idx]\n",
    "E_train_full = E_numpy[idx]\n",
    "F_train_full = F_numpy[idx]\n",
    "r_train_full = r_numpy[idx]\n",
    "dD_dr_train_full = dD_dr_numpy[idx]\n",
    "\n",
    "# Standardize input for improved learning. Fit is done only on training data,\n",
    "# scaling is applied to both descriptors and their derivatives on training and\n",
    "# test sets.\n",
    "scaler = StandardScaler().fit(D_train_full)\n",
    "D_train_full = scaler.transform(D_train_full)\n",
    "D_whole = scaler.transform(D_numpy)\n",
    "dD_dr_whole = dD_dr_numpy / scaler.scale_[None, None, None, :]\n",
    "dD_dr_train_full = dD_dr_train_full / scaler.scale_[None, None, None, :]\n",
    "\n",
    "# Calculate the variance of energy and force values for the training set. These\n",
    "# are used to balance their contribution to the MSE loss\n",
    "var_energy_train = E_train_full.var()\n",
    "var_force_train = F_train_full.var()\n",
    "\n",
    "# Subselect 20% of validation points for early stopping.\n",
    "D_train, D_valid, E_train, E_valid, F_train, F_valid, dD_dr_train, dD_dr_valid = train_test_split(\n",
    "    D_train_full,\n",
    "    E_train_full,\n",
    "    F_train_full,\n",
    "    dD_dr_train_full,\n",
    "    test_size=0.2,\n",
    "    random_state=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ase\n",
    "from ase.calculators.lj import LennardJones\n",
    "import matplotlib.pyplot as plt\n",
    "from dscribe.descriptors import SOAP\n",
    "\n",
    "# Setting up the SOAP descriptor\n",
    "soap = SOAP(\n",
    "    species=[\"H\"],\n",
    "    periodic=False,\n",
    "    rcut=5.0,\n",
    "    sigma=0.5,\n",
    "    nmax=3,\n",
    "    lmax=0,\n",
    ")\n",
    "\n",
    "# Generate dataset of Lennard-Jones energies and forces\n",
    "n_samples = 200\n",
    "traj = []\n",
    "n_atoms = 2\n",
    "energies = np.zeros(n_samples)\n",
    "forces = np.zeros((n_samples, n_atoms, 3))\n",
    "r = np.linspace(2.5, 5.0, n_samples)\n",
    "for i, d in enumerate(r):\n",
    "    a = ase.Atoms('HH', positions = [[-0.5 * d, 0, 0], [0.5 * d, 0, 0]])\n",
    "    a.set_calculator(LennardJones(epsilon=1.0 , sigma=2.9))\n",
    "    traj.append(a)\n",
    "    energies[i] = a.get_total_energy()\n",
    "    forces[i, :, :] = a.get_forces()\n",
    "\t\n",
    "# Plot the energies to validate them\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "plt.subplots_adjust(left=0.1, right=0.95, top=0.95, bottom=0.1)\n",
    "line, = ax.plot(r, energies)\n",
    "plt.xlabel(\"Distance (Ã…)\")\n",
    "plt.ylabel(\"Energy (eV)\")\n",
    "plt.show()\n",
    "\n",
    "# Create the SOAP desciptors and their derivatives for all samples. One center\n",
    "# is chosen to be directly between the atoms.\n",
    "derivatives, descriptors = soap.derivatives(\n",
    "    traj,\n",
    "    positions=[[[0, 0, 0]]] * len(r),\n",
    "    method=\"analytical\"\n",
    ")\n",
    "\n",
    "# Save to disk for later training\n",
    "np.save(\"r.npy\", r)\n",
    "np.save(\"E.npy\", energies)\n",
    "np.save(\"D.npy\", descriptors)\n",
    "np.save(\"dD_dr.npy\", derivatives)\n",
    "np.save(\"F.npy\", forces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1, 6)\n"
     ]
    }
   ],
   "source": [
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# Load the dataset\n",
    "D_numpy = np.load(\"D.npy\")[:, 0, :]  # We only have one SOAP center\n",
    "n_samples, n_features = D_numpy.shape\n",
    "E_numpy = np.array([np.load(\"E.npy\")]).T\n",
    "F_numpy = np.load(\"F.npy\")\n",
    "dD_dr_numpy = np.load(\"dD_dr.npy\")[:, 0, :, :, :]  # We only have one SOAP center\n",
    "r_numpy = np.load(\"r.npy\")\n",
    "\n",
    "# Select equally spaced points for training\n",
    "n_train = 30\n",
    "idx = np.linspace(0, len(r_numpy) - 1, n_train).astype(int)\n",
    "D_train_full = D_numpy[idx]\n",
    "E_train_full = E_numpy[idx]\n",
    "F_train_full = F_numpy[idx]\n",
    "r_train_full = r_numpy[idx]\n",
    "dD_dr_train_full = dD_dr_numpy[idx]\n",
    "\n",
    "# Standardize input for improved learning. Fit is done only on training data,\n",
    "# scaling is applied to both descriptors and their derivatives on training and\n",
    "# test sets.\n",
    "scaler = StandardScaler().fit(D_train_full)\n",
    "D_train_full = scaler.transform(D_train_full)\n",
    "D_whole = scaler.transform(D_numpy)\n",
    "dD_dr_whole = dD_dr_numpy / scaler.scale_[None, None, None, :]\n",
    "dD_dr_train_full = dD_dr_train_full / scaler.scale_[None, None, None, :]\n",
    "\n",
    "# Calculate the variance of energy and force values for the training set. These\n",
    "# are used to balance their contribution to the MSE loss\n",
    "var_energy_train = E_train_full.var()\n",
    "var_force_train = F_train_full.var()\n",
    "\n",
    "# Subselect 20% of validation points for early stopping.\n",
    "D_train, D_valid, E_train, E_valid, F_train, F_valid, dD_dr_train, dD_dr_valid = train_test_split(\n",
    "    D_train_full,\n",
    "    E_train_full,\n",
    "    F_train_full,\n",
    "    dD_dr_train_full,\n",
    "    test_size=0.2,\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "# Create tensors for pytorch\n",
    "D_whole = torch.Tensor(D_whole)\n",
    "D_train = torch.Tensor(D_train)\n",
    "D_valid = torch.Tensor(D_valid)\n",
    "E_train = torch.Tensor(E_train)\n",
    "E_valid = torch.Tensor(E_valid)\n",
    "F_train = torch.Tensor(F_train)\n",
    "F_valid = torch.Tensor(F_valid)\n",
    "dD_dr_train = torch.Tensor(dD_dr_train)\n",
    "dD_dr_valid = torch.Tensor(dD_dr_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNet(torch.nn.Module):\n",
    "    \"\"\"A simple feed-forward network with one hidden layer, randomly\n",
    "    initialized weights, sigmoid activation and a linear output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_hidden, n_out):\n",
    "        super(FFNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(n_features, n_hidden)\n",
    "        torch.nn.init.normal_(self.linear1.weight, mean=0, std=1.0)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.linear2 = torch.nn.Linear(n_hidden, n_out)\n",
    "        torch.nn.init.normal_(self.linear2.weight, mean=0, std=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def energy_force_loss(E_pred, E_train, F_pred, F_train):\n",
    "    \"\"\"Custom loss function that targets both energies and forces.\n",
    "    \"\"\"\n",
    "    energy_loss = torch.mean((E_pred - E_train)**2) / var_energy_train\n",
    "    force_loss = torch.mean((F_pred - F_train)**2) / var_force_train\n",
    "    return energy_loss + force_loss\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = FFNet(n_features, n_hidden=5, n_out=1)\n",
    "\n",
    "# The Adam optimizer is used for training the model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished epoch: 0 with loss: 21.220672607421875\n",
      "  Finished epoch: 500 with loss: 0.00022567455016542226\n",
      "  Finished epoch: 1000 with loss: 0.00017745653167366982\n",
      "Early stopping at epoch 1430\n"
     ]
    }
   ],
   "source": [
    "# Train!\n",
    "n_max_epochs = 5000\n",
    "batch_size = 2\n",
    "patience = 20\n",
    "i_worse = 0\n",
    "old_valid_loss = float(\"Inf\")\n",
    "best_valid_loss = float(\"Inf\")\n",
    "\n",
    "# We explicitly require that the gradients should be calculated for the input\n",
    "# variables. PyTorch will not do this by default as it is typically not needed.\n",
    "D_valid.requires_grad = True\n",
    "\n",
    "# Epochs\n",
    "for i_epoch in range(n_max_epochs):\n",
    "\n",
    "    # Batches\n",
    "    permutation = torch.randperm(D_train.size()[0])\n",
    "    for i in range(0, D_train.size()[0], batch_size):\n",
    "\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        D_train_batch, E_train_batch = D_train[indices], E_train[indices]\n",
    "        D_train_batch.requires_grad = True\n",
    "        F_train_batch, dD_dr_train_batch = F_train[indices], dD_dr_train[indices]\n",
    "\n",
    "        # Forward pass: Predict energies from the descriptor input\n",
    "        E_train_pred_batch = model(D_train_batch)\n",
    "\n",
    "        # Get derivatives of model output with respect to input variables. The\n",
    "        # torch.autograd.grad-function can be used for this, as it returns the\n",
    "        # gradients of the input with respect to outputs. It is very important\n",
    "        # to set the create_graph=True in this case. Without it the derivatives\n",
    "        # of the NN parameters with respect to the loss from the force error\n",
    "        # will not be populated (=the force error will not affect the\n",
    "        # training), but the model will still run fine without errors.\n",
    "        df_dD_train_batch = torch.autograd.grad(\n",
    "            outputs=E_train_pred_batch,\n",
    "            inputs=D_train_batch,\n",
    "            grad_outputs=torch.ones_like(E_train_pred_batch),\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        # Get derivatives of input variables (=descriptor) with respect to atom\n",
    "        # positions = forces\n",
    "        F_train_pred_batch = -torch.einsum('ijkl,il->ijk', dD_dr_train_batch, df_dD_train_batch)\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        # D_train_batch.grad.data.zero_()\n",
    "        optimizer.zero_grad()\n",
    "        loss = energy_force_loss(E_train_pred_batch, E_train_batch, F_train_pred_batch, F_train_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Check early stopping criterion and save best model\n",
    "    E_valid_pred = model(D_valid)\n",
    "    df_dD_valid = torch.autograd.grad(\n",
    "        outputs=E_valid_pred,\n",
    "        inputs=D_valid,\n",
    "        grad_outputs=torch.ones_like(E_valid_pred),\n",
    "    )[0]\n",
    "    F_valid_pred = -torch.einsum('ijkl,il->ijk', dD_dr_valid, df_dD_valid)\n",
    "    valid_loss = energy_force_loss(E_valid_pred, E_valid, F_valid_pred, F_valid)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        # print(\"Saving at epoch {}\".format(i_epoch))\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        best_valid_loss = valid_loss\n",
    "    if valid_loss >= old_valid_loss:\n",
    "        i_worse += 1\n",
    "    else:\n",
    "        i_worse = 0\n",
    "    if i_worse > patience:\n",
    "        print(\"Early stopping at epoch {}\".format(i_epoch))\n",
    "        break\n",
    "    old_valid_loss = valid_loss\n",
    "\n",
    "    if i_epoch % 500 == 0:\n",
    "        print(\"  Finished epoch: {} with loss: {}\".format(i_epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "torch.Size([2, 2, 3, 6])\n",
      "torch.Size([2, 6])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(E_train_pred_batch.shape)\n",
    "print(dD_dr_train_batch.shape)\n",
    "print(df_dD_train_batch.shape)\n",
    "print(F_train_pred_batch.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3136],\n",
      "        [-0.4304]], grad_fn=<AddmmBackward>)\n",
      "tensor([[[[-0.5274,  0.6357, -0.3488, -0.4584, -0.0762,  1.3457],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.5274, -0.6357,  0.3488,  0.4584,  0.0762, -1.3457],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.4582,  0.8208, -0.7472, -0.8145,  0.2208,  1.4606],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.4582, -0.8208,  0.7472,  0.8145, -0.2208, -1.4606],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n",
      "tensor([[-1.9331,  0.1829,  0.9357,  3.2716,  0.2745,  0.1974],\n",
      "        [-2.0175,  0.3933,  0.6060,  2.9655,  0.2819,  0.7130]],\n",
      "       grad_fn=<MmBackward>)\n",
      "tensor([[[ 0.4455, -0.0000, -0.0000],\n",
      "         [-0.4455, -0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.5173, -0.0000, -0.0000],\n",
      "         [-0.5173, -0.0000, -0.0000]]], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(E_train_pred_batch)\n",
    "print(dD_dr_train_batch)\n",
    "print(df_dD_train_batch)\n",
    "print(F_train_pred_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
